# YOLOv5 üöÄ –æ—Ç Ultralytics, –ª–∏—Ü–µ–Ω–∑–∏—è GPL-3.0
"""
–£—Ç–∏–ª–∏—Ç—ã PyTorch
"""

import math
import os
import platform
import subprocess
import time
import warnings
from contextlib import contextmanager
from copy import deepcopy
from pathlib import Path

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parallel import DistributedDataParallel as DDP

from utils.general import LOGGER, check_version, colorstr, file_date, git_describe

LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
RANK = int(os.getenv('RANK', -1))
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))

try:
    import thop  # –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è FLOPs
except ImportError:
    thop = None

# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π PyTorch
warnings.filterwarnings('ignore', message='User provided device_type of \'cuda\', but CUDA is not available. Disabling')
warnings.filterwarnings('ignore', category=UserWarning)


def smart_inference_mode(torch_1_9=check_version(torch.__version__, '1.9.0')):
    # –ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä torch.inference_mode(), –µ—Å–ª–∏ torch>=1.9.0, –∏–Ω–∞—á–µ torch.no_grad()
    def decorate(fn):
        return (torch.inference_mode if torch_1_9 else torch.no_grad)()(fn)

    return decorate


def smartCrossEntropyLoss(label_smoothing=0.0):
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç nn.CrossEntropyLoss —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º label smoothing –¥–ª—è torch>=1.10.0
    if check_version(torch.__version__, '1.10.0'):
        return nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    if label_smoothing > 0:
        LOGGER.warning(f'WARNING ‚ö†Ô∏è label smoothing {label_smoothing} —Ç—Ä–µ–±—É–µ—Ç torch>=1.10.0')
    return nn.CrossEntropyLoss()


def smart_DDP(model):
    # –°–æ–∑–¥–∞–Ω–∏–µ DDP –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
    assert not check_version(torch.__version__, '1.12.0', pinned=True), \
        'torch==1.12.0 torchvision==0.13.0 DDP –æ–±—É—á–µ–Ω–∏–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∏–∑-–∑–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã. ' \
        '–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±–Ω–æ–≤–∏—Ç–µ –∏–ª–∏ –ø–æ–Ω–∏–∑—å—Ç–µ –≤–µ—Ä—Å–∏—é torch. –°–º. https://github.com/ultralytics/yolov5/issues/8395'
    if check_version(torch.__version__, '1.11.0'):
        return DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK, static_graph=True)
    else:
        return DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)


def reshape_classifier_output(model, n=1000):
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ TorchVision –¥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤ 'n', –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    from models.common import Classify
    name, m = list((model.model if hasattr(model, 'model') else model).named_children())[-1]  # –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–æ–¥—É–ª—å
    if isinstance(m, Classify):  # –≥–æ–ª–æ–≤–∞ YOLOv5 Classify()
        if m.linear.out_features != n:
            m.linear = nn.Linear(m.linear.in_features, n)
    elif isinstance(m, nn.Linear):  # ResNet, EfficientNet
        if m.out_features != n:
            setattr(model, name, nn.Linear(m.in_features, n))
    elif isinstance(m, nn.Sequential):
        types = [type(x) for x in m]
        if nn.Linear in types:
            i = types.index(nn.Linear)  # –∏–Ω–¥–µ–∫—Å nn.Linear
            if m[i].out_features != n:
                m[i] = nn.Linear(m[i].in_features, n)
        elif nn.Conv2d in types:
            i = types.index(nn.Conv2d)  # –∏–Ω–¥–µ–∫—Å nn.Conv2d
            if m[i].out_channels != n:
                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    # –î–µ–∫–æ—Ä–∞—Ç–æ—Ä, –∑–∞—Å—Ç–∞–≤–ª—è—é—â–∏–π –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∂–¥–∞—Ç—å, –ø–æ–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∞—Å—Ç–µ—Ä —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞–µ—Ç
    if local_rank not in [-1, 0]:
        dist.barrier(device_ids=[local_rank])
    yield
    if local_rank == 0:
        dist.barrier(device_ids=[0])


def device_count():
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö CUDA-—É—Å—Ç—Ä–æ–π—Å—Ç–≤. –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è torch.cuda.device_count(). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Linux –∏ Windows
    assert platform.system() in ('Linux', 'Windows'), 'device_count() –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ Linux –∏–ª–∏ Windows'
    try:
        cmd = 'nvidia-smi -L | wc -l' if platform.system() == 'Linux' else 'nvidia-smi -L | find /c /v ""'  # Windows
        return int(subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1])
    except Exception:
        return 0


def select_device(device='', batch_size=0, newline=True):
    # device = None –∏–ª–∏ 'cpu' –∏–ª–∏ 0 –∏–ª–∏ '0' –∏–ª–∏ '0,1,2,3'
    s = f'YOLOv5 üöÄ {git_describe() or file_date()} Python-{platform.python_version()} torch-{torch.__version__} '
    device = str(device).strip().lower().replace('cuda:', '').replace('none', '')  # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É, 'cuda:0' –≤ '0'
    cpu = device == 'cpu'
    mps = device == 'mps'  # Apple Metal Performance Shaders (MPS)
    if cpu or mps:
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å torch.cuda.is_available() = False
    elif device:  # –∑–∞–ø—Ä–æ—à–µ–Ω–æ –Ω–µ–≥–æ—Ç–æ–≤–æ–µ –∫ CPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ä–µ–¥—ã - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–µ—Ä–µ–¥ assert is_available()
        assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \
            f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π CUDA '--device {device}' –∑–∞–ø—Ä–æ—à–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ '--device cpu' –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"

    if not cpu and not mps and torch.cuda.is_available():  # –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7
        n = len(devices)  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        if n > 1 and batch_size > 0:  # –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫—Ä–∞—Ç–Ω–æ –ª–∏ batch_size –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            assert batch_size % n == 0, f'—Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ {batch_size} –Ω–µ –∫—Ä–∞—Ç–µ–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤—É GPU {n}'
        space = ' ' * (len(s) + 1)
        for i, d in enumerate(devices):
            p = torch.cuda.get_device_properties(i)
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\n"  # –±–∞–π—Ç—ã –≤ MB
        arg = 'cuda:0'
    elif mps and getattr(torch, 'has_mps', False) and torch.backends.mps.is_available():  # –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ MPS, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        s += 'MPS\n'
        arg = 'mps'
    else:  # –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ CPU
        s += 'CPU\n'
        arg = 'cpu'

    if not newline:
        s = s.rstrip()
    LOGGER.info(s)
    return torch.device(arg)


def time_sync():
    # –¢–æ—á–Ω–æ–µ –≤—Ä–µ–º—è PyTorch
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()


def profile(input, ops, n=10, device=None):
    """ –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏/–ø–∞–º—è—Ç–∏/FLOPs YOLOv5
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        input = torch.randn(16, 3, 640, 640)
        m1 = lambda x: x * torch.sigmoid(x)
        m2 = nn.SiLU()
        profile(input, [m1, m2], n=100)  # –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π
    """
    results = []
    if not isinstance(device, torch.device):
        device = select_device(device)
    print(f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
          f"{'input':>24s}{'output':>24s}")

    for x in input if isinstance(input, list) else [input]:
        x = x.to(device)
        x.requires_grad = True
        for m in ops if isinstance(ops, list) else [ops]:
            m = m.to(device) if hasattr(m, 'to') else m  # —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m
            tf, tb, t = 0, 0, [0, 0, 0]  # dt forward, backward
            try:
                flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # GFLOPs
            except Exception:
                flops = 0

            try:
                for _ in range(n):
                    t[0] = time_sync()
                    y = m(x)
                    t[1] = time_sync()
                    try:
                        _ = (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                        t[2] = time_sync()
                    except Exception:  # –Ω–µ—Ç –º–µ—Ç–æ–¥–∞ backward
                        # print(e)  # –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        t[2] = float('nan')
                    tf += (t[1] - t[0]) * 1000 / n  # ms per op forward
                    tb += (t[2] - t[1]) * 1000 / n  # ms per op backward
                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)
                s_in, s_out = (tuple(x.shape) if isinstance(x, torch.Tensor) else 'list' for x in (x, y))  # —Ñ–æ—Ä–º—ã
                p = sum(x.numel() for x in m.parameters()) if isinstance(m, nn.Module) else 0  # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                print(f'{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}')
                results.append([p, flops, mem, tf, tb, s_in, s_out])
            except Exception as e:
                print(e)
                results.append(None)
            torch.cuda.empty_cache()
    return results


def is_parallel(model):
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —è–≤–ª—è–µ—Ç—Å—è —Ç–∏–ø–æ–º DP –∏–ª–∏ DDP
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)


def de_parallel(model):
    # –î–µ–ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω–æ-GPU –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —è–≤–ª—è–µ—Ç—Å—è —Ç–∏–ø–æ–º DP –∏–ª–∏ DDP
    return model.module if is_parallel(model) else model


def initialize_weights(model):
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:
            m.eps = 1e-3
            m.momentum = 0.03
        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:
            m.inplace = True


def find_modules(model, mclass=nn.Conv2d):
    # –ù–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–µ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å—É –º–æ–¥—É–ª—è 'mclass'
    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]


def sparsity(model):
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é —Ä–µ–¥–∫–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
    a, b = 0, 0
    for p in model.parameters():
        a += p.numel()
        b += (p == 0).sum()
    return b / a


def prune(model, amount=0.3):
    # –ü—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º–æ–π –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ä–µ–¥–∫–æ—Å—Ç–∏
    import torch.nn.utils.prune as prune
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            prune.l1_unstructured(m, name='weight', amount=amount)  # –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ
            prune.remove(m, 'weight')  # —Å–¥–µ–ª–∞—Ç—å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º
    LOGGER.info(f'–ú–æ–¥–µ–ª—å –ø—Ä–æ—Ä–µ–∂–µ–Ω–∞ –¥–æ {sparsity(model):.3g} –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ä–µ–¥–∫–æ—Å—Ç–∏')


def fuse_conv_and_bn(conv, bn):
    # –°–ª–∏—è–Ω–∏–µ —Å–ª–æ–µ–≤ Conv2d() –∏ BatchNorm2d() https://tehnokv.com/posts/fusing-batchnorm-and-conv/
    fusedconv = nn.Conv2d(conv.in_channels,
                          conv.out_channels,
                          kernel_size=conv.kernel_size,
                          stride=conv.stride,
                          padding=conv.padding,
                          dilation=conv.dilation,
                          groups=conv.groups,
                          bias=True).requires_grad_(False).to(conv.weight.device)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è
    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv


def model_info(model, verbose=False, imgsz=640):
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏. img_size –º–æ–∂–µ—Ç –±—ã—Ç—å int –∏–ª–∏ —Å–ø–∏—Å–æ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä img_size=640 –∏–ª–∏ img_size=[640, 320]
    n_p = sum(x.numel() for x in model.parameters())  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    if verbose:
        print(f"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}")
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace('module_list.', '')
            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %
                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))

    try:  # FLOPs
        p = next(model.parameters())
        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π stride
        im = torch.empty((1, p.shape[1], stride, stride), device=p.device)  # –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ BCHW
        flops = thop.profile(deepcopy(model), inputs=(im,), verbose=False)[0] / 1E9 * 2  # stride GFLOPs
        imgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz]  # —Ä–∞—Å—à–∏—Ä–∏—Ç—å, –µ—Å–ª–∏ int/float
        fs = f', {flops * imgsz[0] / stride * imgsz[1] / stride:.1f} GFLOPs'  # 640x640 GFLOPs
    except Exception:
        fs = ''

    name = Path(model.yaml_file).stem.replace('yolov5', 'YOLOv5') if hasattr(model, 'yaml_file') else 'Model'
    LOGGER.info(f"{name} summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}")


def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ img(bs,3,y,x) —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –∫—Ä–∞—Ç–Ω–æ—Å—Ç—å gs
    if ratio == 1.0:
        return img
    h, w = img.shape[2:]
    s = (int(h * ratio), int(w * ratio))  # –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
    img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
    if not same_shape:  # –∑–∞–ø–æ–ª–Ω–∏—Ç—å/–æ–±—Ä–µ–∑–∞—Ç—å img
        h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # –∑–Ω–∞—á–µ–Ω–∏–µ = —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ImageNet


def copy_attr(a, b, include=(), exclude=()):
    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏–∑ b –≤ a, —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–∫–ª—é—á–µ–Ω–∏—è [...] –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è [...]
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue
        else:
            setattr(a, k, v)


def smart_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):
    # YOLOv5 3-–≥—Ä—É–ø–ø–æ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: 0) –≤–µ—Å–∞ —Å decadence, 1) –≤–µ—Å–∞ –±–µ–∑ decadence, 2) —Å–º–µ—â–µ–Ω–∏—è –±–µ–∑ decadence
    g = [], [], []  # –≥—Ä—É–ø–ø—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # —Å–ª–æ–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä BatchNorm2d()
    for v in model.modules():
        for p_name, p in v.named_parameters(recurse=0):
            if p_name == 'bias':  # —Å–º–µ—â–µ–Ω–∏–µ (–±–µ–∑ decadence)
                g[2].append(p)
            elif p_name == 'weight' and isinstance(v, bn):  # –≤–µ—Å (–±–µ–∑ decadence)
                g[1].append(p)
            else:
                g[0].append(p)  # –≤–µ—Å (—Å decadence)

    if name == 'Adam':
        optimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # –∏–∑–º–µ–Ω–∏—Ç—å beta1 –Ω–∞ momentum
    elif name == 'AdamW':
        optimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)
    elif name == 'RMSProp':
        optimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)
    elif name == 'SGD':
        optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)
    else:
        raise NotImplementedError(f'–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä {name} –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω.')

    optimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # –¥–æ–±–∞–≤–∏—Ç—å g0 —Å weight_decay
    optimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # –¥–æ–±–∞–≤–∏—Ç—å g1 (–≤–µ—Å–∞ BatchNorm2d)
    LOGGER.info(f"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}) —Å –≥—Ä—É–ø–ø–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ "
                f"{len(g[1])} –≤–µ—Å(decay=0.0), {len(g[0])} –≤–µ—Å(decay={decay}), {len(g[2])} —Å–º–µ—â–µ–Ω–∏–µ")
    return optimizer


def smart_hub_load(repo='ultralytics/yolov5', model='yolov5s', **kwargs):
    # –û–±–æ–ª–æ—á–∫–∞ –¥–ª—è torch.hub.load() —Å —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø—Ä–æ–±–ª–µ–º
    if check_version(torch.__version__, '1.9.1'):
        kwargs['skip_validation'] = True  # –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –ª–∏–º–∏—Ç–∞ API GitHub
    if check_version(torch.__version__, '1.12.0'):
        kwargs['trust_repo'] = True  # –∞—Ä–≥—É–º–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—á–∏–Ω–∞—è —Å torch 0.12
    try:
        return torch.hub.load(repo, model, **kwargs)
    except Exception:
        return torch.hub.load(repo, model, force_reload=True, **kwargs)


def smart_resume(ckpt, optimizer, ema=None, weights='yolov5s.pt', epochs=300, resume=True):
    # –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∏–∑ —á–∞—Å—Ç–∏—á–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    best_fitness = 0.0
    start_epoch = ckpt['epoch'] + 1
    if ckpt['optimizer'] is not None:
        optimizer.load_state_dict(ckpt['optimizer'])  # –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        best_fitness = ckpt['best_fitness']
    if ema and ckpt.get('ema'):
        ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA
        ema.updates = ckpt['updates']
    if resume:
        assert start_epoch > 0, f'{weights} –æ–±—É—á–µ–Ω–∏–µ –¥–æ {epochs} —ç–ø–æ—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–µ—á–µ–≥–æ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è—Ç—å.\n' \
                                f"–ù–∞—á–Ω–∏—Ç–µ –Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ --resume, –Ω–∞–ø—Ä–∏–º–µ—Ä 'python train.py --weights {weights}'"
        LOGGER.info(f'–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∏–∑ {weights} —Å {start_epoch} —ç–ø–æ—Ö–∏ –¥–æ {epochs} –æ–±—â–∏—Ö —ç–ø–æ—Ö')
    if epochs < start_epoch:
        LOGGER.info(f"{weights} –æ–±—É—á–µ–Ω–æ –Ω–∞ {ckpt['epoch']} —ç–ø–æ—Ö. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ {epochs} —ç–ø–æ—Ö.")
        epochs += ckpt['epoch']  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–ø–æ—Ö–∏
    return best_fitness, start_epoch, epochs


class EarlyStopping:
    # –ü—Ä–æ—Å—Ç–æ–π Early Stopper –¥–ª—è YOLOv5
    def __init__(self, patience=30):
        self.best_fitness = 0.0  # –Ω–∞–ø—Ä–∏–º–µ—Ä, mAP
        self.best_epoch = 0
        self.patience = patience or float('inf')  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è fitness
        self.possible_stop = False  # –≤–æ–∑–º–æ–∂–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–µ

    def __call__(self, epoch, fitness):
        if fitness >= self.best_fitness:  # >= 0 –¥–ª—è —É—á–µ—Ç–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º fitness
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
        self.possible_stop = delta >= (self.patience - 1)  # –≤–æ–∑–º–æ–∂–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–µ
        stop = delta >= self.patience  # –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–æ –æ–∂–∏–¥–∞–Ω–∏–µ
        if stop:
            LOGGER.info(f'–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–π –≤ —Ç–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {self.patience} —ç–ø–æ—Ö. '
                        f'–õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –Ω–∞ —ç–ø–æ—Ö–µ {self.best_epoch}, –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ best.pt.\n'
                        f'–ß—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å EarlyStopping(patience={self.patience}) –ø–µ—Ä–µ–¥–∞–π—Ç–µ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è, '
                        f'–Ω–∞–ø—Ä–∏–º–µ—Ä, `python train.py --patience 300` –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `--patience 0`, —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å EarlyStopping.')
        return stop


class ModelEMA:
    """ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π Exponential Moving Average (EMA) –∏–∑ https://github.com/rwightman/pytorch-image-models
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ state_dict –º–æ–¥–µ–ª–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –±—É—Ñ–µ—Ä–æ–≤)
    –î–ª—è –¥–µ—Ç–∞–ª–µ–π EMA —Å–º. https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage
    """

    def __init__(self, model, decay=0.9999, tau=2000, updates=0):
        # –°–æ–∑–¥–∞–Ω–∏–µ EMA
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        self.updates = updates  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π EMA
        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞—Ç—É—Ö–∞–Ω–∏—è (–¥–ª—è –ø–æ–º–æ—â–∏ –≤ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —ç–ø–æ—Ö–∞—Ö)
        for p in self.ema.parameters():
            p.requires_grad_(False)

    def update(self, model):
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ EMA
        self.updates += 1
        d = self.decay(self.updates)

        msd = de_parallel(model).state_dict()  # state_dict –º–æ–¥–µ–ª–∏
        for k, v in self.ema.state_dict().items():
            if v.dtype.is_floating_point:  # –≤–µ—Ä–Ω–æ –¥–ª—è FP16 –∏ FP32
                v *= d
                v += (1 - d) * msd[k].detach()
        # assert v.dtype == msd[k].dtype == torch.float32, f'{k}: EMA {v.dtype} –∏ –º–æ–¥–µ–ª—å {msd[k].dtype} –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å FP32'

    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ EMA
        copy_attr(self.ema, model, include, exclude)