# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license
"""
Run YOLOv5 classification inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python classify/predict.py --weights yolov5s-cls.pt --source 0                               # webcam
                                                                   img.jpg                         # image
                                                                   vid.mp4                         # video
                                                                   screen                          # screenshot
                                                                   path/                           # directory
                                                                   'path/*.jpg'                    # glob
                                                                   'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                                   'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python classify/predict.py --weights yolov5s-cls.pt                 # PyTorch
                                           yolov5s-cls.torchscript        # TorchScript
                                           yolov5s-cls.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                           yolov5s-cls_openvino_model     # OpenVINO
                                           yolov5s-cls.engine             # TensorRT
                                           yolov5s-cls.mlmodel            # CoreML (macOS-only)
                                           yolov5s-cls_saved_model        # TensorFlow SavedModel
                                           yolov5s-cls.pb                 # TensorFlow GraphDef
                                           yolov5s-cls.tflite             # TensorFlow Lite
                                           yolov5s-cls_edgetpu.tflite     # TensorFlow Edge TPU
                                           yolov5s-cls_paddle_model       # PaddlePaddle
"""

import argparse
import os
import platform
import sys
from pathlib import Path

import torch
import torch.nn.functional as F

# –ü–æ–ª—É—á–∏—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
FILE = Path(__file__).resolve()
# –ü–æ–ª—É—á–∏—Ç—å –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é YOLOv5
ROOT = FILE.parents[1]
# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—É—Ç—å, –µ—Å–ª–∏ –µ–µ —Ç–∞–º –Ω–µ—Ç
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))
# –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))

# –ò–º–ø–æ—Ä—Ç –∫–ª–∞—Å—Å–∞ DetectMultiBackend –∏–∑ –º–æ–¥—É–ª—è models.common
from models.common import DetectMultiBackend
# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–∏ classify_transforms –∏–∑ –º–æ–¥—É–ª—è utils.augmentations
from utils.augmentations import classify_transforms
# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –º–æ–¥—É–ª—è utils.dataloaders
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
# –ò–º–ø–æ—Ä—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ –º–æ–¥—É–ª—è utils.general
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, print_args, strip_optimizer)
# –ò–º–ø–æ—Ä—Ç –∫–ª–∞—Å—Å–∞ Annotator –∏–∑ –º–æ–¥—É–ª—è utils.plots
from utils.plots import Annotator
# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –º–æ–¥—É–ª—è utils.torch_utils
from utils.torch_utils import select_device, smart_inference_mode

# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –≤—ã–≤–æ–¥–∞
@smart_inference_mode()
def run(
        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≤–µ—Å–∞–º–∏ –º–æ–¥–µ–ª–∏
        weights=ROOT / 'yolov5s-cls.pt',
        # –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö (—Ñ–∞–π–ª, –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, URL –∏ —Ç.–¥.)
        source=ROOT / 'data/images',
        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        data=ROOT / 'data/coco128.yaml',
        # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ (–≤—ã—Å–æ—Ç–∞, —à–∏—Ä–∏–Ω–∞)
        imgsz=(224, 224),
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ CUDA –∏–ª–∏ CPU –¥–ª—è –≤—ã–≤–æ–¥–∞
        device='',
        # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–≤–æ–¥–∞
        view_img=False,
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        save_txt=False,
        # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/–≤–∏–¥–µ–æ
        nosave=False,
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        augment=False,
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏
        visualize=False,
        # –û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏
        update=False,
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        project=ROOT / 'runs/predict-cls',
        # –ò–º—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        name='exp',
        # –†–∞–∑—Ä–µ—à–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        exist_ok=False,
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–æ–≤–∏–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (FP16) –¥–ª—è –≤—ã–≤–æ–¥–∞
        half=False,
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenCV DNN –¥–ª—è –≤—ã–≤–æ–¥–∞ ONNX –º–æ–¥–µ–ª–µ–π
        dnn=False,
        # –®–∞–≥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤
        vid_stride=1,
):
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Å—Ç—Ä–æ–∫—É
    source = str(source)
    # –§–ª–∞–≥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    save_img = not nosave and not source.endswith('.txt')
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —Ñ–∞–π–ª–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –≤–∏–¥–µ–æ
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ URL
    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤–µ–±-–∫–∞–º–µ—Ä–æ–π, —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–∞–π–ª–æ–º –∏–ª–∏ URL
    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–º
    screenshot = source.lower().startswith('screen')
    # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ - URL –∏ —Ñ–∞–π–ª, —Ç–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª
    if is_url and is_file:
        source = check_file(source)

    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)
    # –°–æ–∑–¥–∞—Ç—å –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–µ—Ç–æ–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)

    # –í—ã–±—Ä–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã–≤–æ–¥–∞
    device = select_device(device)
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    # –ü–æ–ª—É—á–∏—Ç—å —à–∞–≥, –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ–ª–∞–≥ PyTorch –º–æ–¥–µ–ª–∏
    stride, names, pt = model.stride, model.names, model.pt
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    imgsz = check_img_size(imgsz, s=stride)

    # Dataloader
    # –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞
    bs = 1
    # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ - –≤–µ–±-–∫–∞–º–µ—Ä–∞
    if webcam:
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        view_img = check_imshow(warn=True)
        # –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤
        dataset = LoadStreams(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)
        # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤
        bs = len(dataset)
    # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ - —Å–∫—Ä–∏–Ω—à–æ—Ç
    elif screenshot:
        # –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)
    # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
    else:
        # –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        dataset = LoadImages(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø—É—Ç–µ–π –∫ –≤–∏–¥–µ–æ –∏ –≤–∏–¥–µ–æ-–ø–∏—Å–∞—Ç–µ–ª–µ–π
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    # –ü—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –ø—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    seen = 0
    # –°–ø–∏—Å–æ–∫ –æ–∫–æ–Ω –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    windows = []
    # –û–±—ä–µ–∫—Ç—ã –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
    dt = (Profile(), Profile(), Profile())
    # –ò—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    for path, im, im0s, vid_cap, s in dataset:
        with dt[0]:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            im = torch.Tensor(im).to(model.device)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            im = im.half() if model.fp16 else im.float()
            # –î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–∞–∫–µ—Ç–∞, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
            if len(im.shape) == 3:
                im = im[None]

                # Inference
        with dt[1]:
            # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏
            results = model(im)

        # Post-process
        with dt[2]:
            # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            pred = F.softmax(results, dim=1)

            # Process predictions
        # –ò—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        for i, prob in enumerate(pred):
            # –£–≤–µ–ª–∏—á–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            seen += 1
            # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ - –≤–µ–±-–∫–∞–º–µ—Ä–∞
            if webcam:
                p, im0, frame = path[i], im0s[i].copy(), dataset.count
                s += f'{i}: '
            # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
            else:
                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ø—É—Ç—å –≤ –æ–±—ä–µ–∫—Ç Path
            p = Path(p)
            # –ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            save_path = str(save_dir / p.name)
            # –ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')

            # –î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫—É –≤—ã–≤–æ–¥–∞
            s += '%gx%g ' % im.shape[2:]
            # –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            annotator = Annotator(im0, example=str(names), pil=True)

            # Print results
            # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
            top5i = prob.argsort(0, descending=True)[:5].tolist()
            s += f"{', '.join(f'{names[j]} {prob[j]:.2f}' for j in top5i)}, "

            # Write results
            # –°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª –∏–ª–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
            text = '\n'.join(f'{prob[j]:.2f} {names[j]}' for j in top5i)
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ –ø–æ–∫–∞–∑–∞—Ç—å –µ–≥–æ
            if save_img or view_img:
                # –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                annotator.text((32, 32), text, txt_color=(255, 255, 255))
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
            if save_txt:
                # –û—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª –∏ –∑–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç
                with open(f'{txt_path}.txt', 'a') as f:
                    f.write(text + '\n')

            # Stream results
            # –ü–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
            im0 = annotator.result()
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if view_img:
                # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–∞ Linux –∏ –æ–∫–Ω–æ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω–æ
                if platform.system() == 'Linux' and p not in windows:
                    # –î–æ–±–∞–≤–∏—Ç—å –æ–∫–Ω–æ –≤ —Å–ø–∏—Å–æ–∫
                    windows.append(p)
                    # –°–æ–∑–¥–∞—Ç—å –æ–∫–Ω–æ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)
                    # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞
                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])
                # –ü–æ–∫–∞–∑–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ–∫–Ω–µ
                cv2.imshow(str(p), im0)
                # –ü–æ–¥–æ–∂–¥–∞—Ç—å 1 –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—É
                cv2.waitKey(1)

                # Save results (image with detections)
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if save_img:
                # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if dataset.mode == 'image':
                    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    cv2.imwrite(save_path, im0)
                # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ (–≤–∏–¥–µ–æ –∏–ª–∏ –ø–æ—Ç–æ–∫)
                else:
                    # –ï—Å–ª–∏ –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ –∏–∑–º–µ–Ω–∏–ª—Å—è
                    if vid_path[i] != save_path:
                        # –û–±–Ω–æ–≤–∏—Ç—å –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ
                        vid_path[i] = save_path
                        # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –ø–∏—Å–∞—Ç–µ–ª—å —É–∂–µ —Å–æ–∑–¥–∞–Ω, –∑–∞–∫—Ä—ã—Ç—å –µ–≥–æ
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            vid_writer[i].release()
                            # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ - –≤–∏–¥–µ–æ
                        if vid_cap:
                            # –ü–æ–ª—É—á–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –∫–∞–¥—Ä–æ–≤
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            # –ü–æ–ª—É—á–∏—Ç—å —à–∏—Ä–∏–Ω—É –∫–∞–¥—Ä–∞
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            # –ü–æ–ª—É—á–∏—Ç—å –≤—ã—Å–æ—Ç—É –∫–∞–¥—Ä–∞
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ (–ø–æ—Ç–æ–∫)
                        else:
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ .mp4
                        save_path = str(Path(save_path).with_suffix('.mp4'))
                        # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –≤–∏–¥–µ–æ –ø–∏—Å–∞—Ç–µ–ª—å
                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                    # –ó–∞–ø–∏—Å–∞—Ç—å –∫–∞–¥—Ä –≤ –≤–∏–¥–µ–æ
                    vid_writer[i].write(im0)

        # Print time (inference-only)
        # –í—ã–≤–µ—Å—Ç–∏ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏
        LOGGER.info(f"{s}{dt[1].dt * 1E3:.1f}ms")

    # Print results
    # –í—ã—á–∏—Å–ª–∏—Ç—å –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    t = tuple(x.t / seen * 1E3 for x in dt)
    # –í—ã–≤–µ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)
    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    if save_txt or save_img:
        # –°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä–æ–∫—É —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–µ—Ç–∫–∞—Ö
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        # –í—ã–≤–µ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å
    if update:
        # –û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å
        strip_optimizer(weights[0])


def parse_opt():
    # –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç –ø–∞—Ä—Å–µ—Ä–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser()
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É —Å –≤–µ—Å–∞–º–∏ –º–æ–¥–µ–ª–∏
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s-cls.pt', help='model path(s)')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[224], help='inference size h,w')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ CUDA –∏–ª–∏ CPU
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–∫–∞–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    parser.add_argument('--view-img', action='store_true', help='show results')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/–≤–∏–¥–µ–æ
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    parser.add_argument('--update', action='store_true', help='update all models')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    parser.add_argument('--project', default=ROOT / 'runs/predict-cls', help='save results to project/name')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –∏–º–µ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    parser.add_argument('--name', default='exp', help='save results to project/name')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (FP16)
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è OpenCV DNN –¥–ª—è –≤—ã–≤–æ–¥–∞ ONNX –º–æ–¥–µ–ª–µ–π
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    # –î–æ–±–∞–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è —à–∞–≥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤
    parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    # –†–∞–∑–æ–±—Ä–∞—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    opt = parser.parse_args()
    # –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω–æ–π
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1
    # –í—ã–≤–µ—Å—Ç–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    print_args(vars(opt))
    return opt


def main(opt):
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º
    check_requirements(exclude=('tensorboard', 'thop'))
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é run —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
    run(**vars(opt))


if __name__ == "__main__":
    # –†–∞–∑–æ–±—Ä–∞—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    opt = parse_opt()
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    main(opt)
